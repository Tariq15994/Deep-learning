{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4000 images belonging to 2 classes.\n",
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from keras.preprocessing import image\n",
    "\n",
    "\n",
    "\n",
    "train_dir = 'train' \n",
    "val_dir = 'valid'\n",
    "test_dir = 'test'\n",
    "\n",
    "\n",
    "\n",
    "train_data = image.ImageDataGenerator(rescale= 1/.255) \n",
    "\n",
    "test_data = image.ImageDataGenerator(rescale= 1/.255)\n",
    "\n",
    "\n",
    "train_gen = train_data.flow_from_directory(train_dir,target_size=(224,224),class_mode=\"binary\")\n",
    "validation_gen = test_data.flow_from_directory(val_dir , \n",
    "                                               target_size= (224,224 ),class_mode=\"binary\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[133.33333    74.5098     62.7451   ]\n",
      "   [125.4902     66.666664   54.901962 ]\n",
      "   [125.4902     66.666664   54.901962 ]\n",
      "   ...\n",
      "   [274.5098    219.60785   215.68628  ]\n",
      "   [254.90196   200.        196.07843  ]\n",
      "   [262.7451    207.84314   203.92157  ]]\n",
      "\n",
      "  [[129.41176    70.588234   58.82353  ]\n",
      "   [125.4902     66.666664   54.901962 ]\n",
      "   [125.4902     66.666664   54.901962 ]\n",
      "   ...\n",
      "   [278.43137   223.52942   219.60785  ]\n",
      "   [262.7451    207.84314   203.92157  ]\n",
      "   [266.66666   211.76471   207.84314  ]]\n",
      "\n",
      "  [[129.41176    70.588234   58.82353  ]\n",
      "   [125.4902     66.666664   54.901962 ]\n",
      "   [125.4902     66.666664   54.901962 ]\n",
      "   ...\n",
      "   [305.88235   250.9804    247.05882  ]\n",
      "   [290.19608   235.29411   231.37254  ]\n",
      "   [294.11765   239.21568   235.29411  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[964.7059    964.7059    956.86273  ]\n",
      "   [964.7059    964.7059    956.86273  ]\n",
      "   [964.7059    964.7059    956.86273  ]\n",
      "   ...\n",
      "   [968.62744   972.549     980.39215  ]\n",
      "   [968.62744   972.549     980.39215  ]\n",
      "   [964.7059    968.62744   976.4706   ]]\n",
      "\n",
      "  [[964.7059    964.7059    956.86273  ]\n",
      "   [964.7059    964.7059    956.86273  ]\n",
      "   [964.7059    964.7059    956.86273  ]\n",
      "   ...\n",
      "   [968.62744   972.549     980.39215  ]\n",
      "   [968.62744   972.549     980.39215  ]\n",
      "   [968.62744   972.549     980.39215  ]]\n",
      "\n",
      "  [[964.7059    964.7059    956.86273  ]\n",
      "   [964.7059    964.7059    956.86273  ]\n",
      "   [964.7059    964.7059    956.86273  ]\n",
      "   ...\n",
      "   [972.549     976.4706    984.3137   ]\n",
      "   [972.549     976.4706    984.3137   ]\n",
      "   [968.62744   972.549     980.39215  ]]]\n",
      "\n",
      "\n",
      " [[[ 74.5098     54.901962   31.37255  ]\n",
      "   [ 90.196075   70.588234   47.058823 ]\n",
      "   [101.960785   82.35294    58.82353  ]\n",
      "   ...\n",
      "   [160.78432   145.09804   133.33333  ]\n",
      "   [145.09804   129.41176   117.64706  ]\n",
      "   [149.0196    133.33333   121.56863  ]]\n",
      "\n",
      "  [[ 90.196075   70.588234   47.058823 ]\n",
      "   [ 94.117645   74.5098     50.980392 ]\n",
      "   [ 82.35294    62.7451     39.215687 ]\n",
      "   ...\n",
      "   [168.62746   152.94118   141.17647  ]\n",
      "   [156.86275   141.17647   129.41176  ]\n",
      "   [149.0196    133.33333   121.56863  ]]\n",
      "\n",
      "  [[101.960785   82.35294    58.82353  ]\n",
      "   [ 90.196075   70.588234   47.058823 ]\n",
      "   [ 70.588234   50.980392   35.294117 ]\n",
      "   ...\n",
      "   [172.54903   156.86275   145.09804  ]\n",
      "   [164.70589   149.0196    137.2549   ]\n",
      "   [152.94118   137.2549    125.4902   ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[596.0784    584.3137    517.64703  ]\n",
      "   [498.0392    474.5098    411.7647   ]\n",
      "   [419.60785   392.15686   321.56863  ]\n",
      "   ...\n",
      "   [862.7451    850.9804    815.6863   ]\n",
      "   [858.82355   847.05884   811.7647   ]\n",
      "   [866.6667    854.902     819.60785  ]]\n",
      "\n",
      "  [[854.902     843.13727   776.4706   ]\n",
      "   [701.9608    682.35297   607.84314  ]\n",
      "   [596.0784    576.4706    501.9608   ]\n",
      "   ...\n",
      "   [847.05884   843.13727   823.5294   ]\n",
      "   [874.5098    870.58826   850.9804   ]\n",
      "   [866.6667    862.7451    847.05884  ]]\n",
      "\n",
      "  [[811.7647    800.        725.4902   ]\n",
      "   [643.13727   631.37256   556.86273  ]\n",
      "   [650.9804    631.37256   556.86273  ]\n",
      "   ...\n",
      "   [847.05884   850.9804    831.37256  ]\n",
      "   [901.9608    901.9608    894.1177   ]\n",
      "   [917.64703   913.72546   905.8823   ]]]\n",
      "\n",
      "\n",
      " [[[760.7843    827.451     388.2353   ]\n",
      "   [756.86273   823.5294    376.47058  ]\n",
      "   [752.94116   827.451     372.549    ]\n",
      "   ...\n",
      "   [274.5098    211.76471   149.0196   ]\n",
      "   [274.5098    211.76471   149.0196   ]\n",
      "   [274.5098    211.76471   149.0196   ]]\n",
      "\n",
      "  [[768.62744   835.2941    396.07843  ]\n",
      "   [764.7059    831.37256   384.31372  ]\n",
      "   [756.86273   831.37256   380.39215  ]\n",
      "   ...\n",
      "   [278.43137   215.68628   152.94118  ]\n",
      "   [278.43137   215.68628   152.94118  ]\n",
      "   [282.35294   219.60785   156.86275  ]]\n",
      "\n",
      "  [[776.4706    839.2157    411.7647   ]\n",
      "   [776.4706    843.13727   403.92157  ]\n",
      "   [772.549     839.2157    392.15686  ]\n",
      "   ...\n",
      "   [286.2745    223.52942   160.78432  ]\n",
      "   [286.2745    223.52942   160.78432  ]\n",
      "   [290.19608   227.45097   164.70589  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[615.6863    505.88235   125.4902   ]\n",
      "   [639.2157    545.098     152.94118  ]\n",
      "   [658.82355   588.2353    164.70589  ]\n",
      "   ...\n",
      "   [266.66666   164.70589   105.882355 ]\n",
      "   [258.82352   184.31372   129.41176  ]\n",
      "   [262.7451    192.15686   145.09804  ]]\n",
      "\n",
      "  [[611.7647    494.11765   141.17647  ]\n",
      "   [627.451     525.4902    145.09804  ]\n",
      "   [647.05884   564.7059    145.09804  ]\n",
      "   ...\n",
      "   [266.66666   164.70589   105.882355 ]\n",
      "   [262.7451    188.23529   133.33333  ]\n",
      "   [262.7451    192.15686   145.09804  ]]\n",
      "\n",
      "  [[627.451     501.9608    168.62746  ]\n",
      "   [627.451     525.4902    152.94118  ]\n",
      "   [647.05884   564.7059    145.09804  ]\n",
      "   ...\n",
      "   [262.7451    172.54903   109.803925 ]\n",
      "   [270.58823   196.07843   141.17647  ]\n",
      "   [266.66666   196.07843   149.0196   ]]]\n",
      "\n",
      "\n",
      " ...\n",
      "\n",
      "\n",
      " [[[717.64703   717.64703   717.64703  ]\n",
      "   [717.64703   717.64703   717.64703  ]\n",
      "   [713.72546   713.72546   713.72546  ]\n",
      "   ...\n",
      "   [494.11765   494.11765   494.11765  ]\n",
      "   [537.2549    537.2549    537.2549   ]\n",
      "   [549.0196    549.0196    549.0196   ]]\n",
      "\n",
      "  [[741.17645   741.17645   741.17645  ]\n",
      "   [737.2549    737.2549    737.2549   ]\n",
      "   [733.3333    733.3333    733.3333   ]\n",
      "   ...\n",
      "   [450.9804    450.9804    450.9804   ]\n",
      "   [494.11765   494.11765   494.11765  ]\n",
      "   [529.41174   529.41174   529.41174  ]]\n",
      "\n",
      "  [[725.4902    725.4902    725.4902   ]\n",
      "   [733.3333    733.3333    733.3333   ]\n",
      "   [741.17645   741.17645   741.17645  ]\n",
      "   ...\n",
      "   [478.43137   478.43137   478.43137  ]\n",
      "   [505.88235   505.88235   505.88235  ]\n",
      "   [521.5686    521.5686    521.5686   ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[650.9804    650.9804    650.9804   ]\n",
      "   [650.9804    650.9804    650.9804   ]\n",
      "   [647.05884   647.05884   647.05884  ]\n",
      "   ...\n",
      "   [596.0784    596.0784    596.0784   ]\n",
      "   [596.0784    596.0784    596.0784   ]\n",
      "   [592.15686   592.15686   592.15686  ]]\n",
      "\n",
      "  [[650.9804    650.9804    650.9804   ]\n",
      "   [650.9804    650.9804    650.9804   ]\n",
      "   [650.9804    650.9804    650.9804   ]\n",
      "   ...\n",
      "   [600.        600.        600.       ]\n",
      "   [596.0784    596.0784    596.0784   ]\n",
      "   [596.0784    596.0784    596.0784   ]]\n",
      "\n",
      "  [[650.9804    650.9804    650.9804   ]\n",
      "   [650.9804    650.9804    650.9804   ]\n",
      "   [650.9804    650.9804    650.9804   ]\n",
      "   ...\n",
      "   [600.        600.        600.       ]\n",
      "   [596.0784    596.0784    596.0784   ]\n",
      "   [596.0784    596.0784    596.0784   ]]]\n",
      "\n",
      "\n",
      " [[[ 39.215687   27.450981    7.8431373]\n",
      "   [ 43.137257   23.529411    7.8431373]\n",
      "   [ 39.215687   19.607843    7.8431373]\n",
      "   ...\n",
      "   [ 98.039215   78.43137    66.666664 ]\n",
      "   [ 98.039215   78.43137    66.666664 ]\n",
      "   [ 98.039215   78.43137    66.666664 ]]\n",
      "\n",
      "  [[ 35.294117   23.529411    3.9215686]\n",
      "   [ 35.294117   23.529411    3.9215686]\n",
      "   [ 35.294117   15.686275    3.9215686]\n",
      "   ...\n",
      "   [101.960785   82.35294    70.588234 ]\n",
      "   [101.960785   82.35294    70.588234 ]\n",
      "   [101.960785   82.35294    70.588234 ]]\n",
      "\n",
      "  [[ 35.294117   23.529411    3.9215686]\n",
      "   [ 35.294117   23.529411    3.9215686]\n",
      "   [ 39.215687   19.607843    7.8431373]\n",
      "   ...\n",
      "   [109.803925   90.196075   78.43137  ]\n",
      "   [113.72549    94.117645   82.35294  ]\n",
      "   [109.803925   90.196075   78.43137  ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[ 82.35294    50.980392   39.215687 ]\n",
      "   [ 82.35294    50.980392   39.215687 ]\n",
      "   [ 78.43137    47.058823   35.294117 ]\n",
      "   ...\n",
      "   [ 90.196075   58.82353    47.058823 ]\n",
      "   [ 98.039215   66.666664   54.901962 ]\n",
      "   [ 94.117645   62.7451     50.980392 ]]\n",
      "\n",
      "  [[ 78.43137    47.058823   35.294117 ]\n",
      "   [ 82.35294    50.980392   39.215687 ]\n",
      "   [ 82.35294    50.980392   39.215687 ]\n",
      "   ...\n",
      "   [ 98.039215   66.666664   54.901962 ]\n",
      "   [109.803925   78.43137    66.666664 ]\n",
      "   [105.882355   74.5098     62.7451   ]]\n",
      "\n",
      "  [[ 82.35294    50.980392   39.215687 ]\n",
      "   [ 86.27451    54.901962   43.137257 ]\n",
      "   [ 86.27451    54.901962   43.137257 ]\n",
      "   ...\n",
      "   [105.882355   74.5098     62.7451   ]\n",
      "   [117.64706    86.27451    74.5098   ]\n",
      "   [113.72549    82.35294    70.588234 ]]]\n",
      "\n",
      "\n",
      " [[[462.7451    360.7843    301.9608   ]\n",
      "   [521.5686    411.7647    356.86273  ]\n",
      "   [556.86273   439.2157    396.07843  ]\n",
      "   ...\n",
      "   [600.        411.7647    333.33334  ]\n",
      "   [588.2353    400.        313.7255   ]\n",
      "   [529.41174   341.17648   254.90196  ]]\n",
      "\n",
      "  [[525.4902    423.52942   372.549    ]\n",
      "   [541.17645   431.37256   384.31372  ]\n",
      "   [537.2549    419.60785   376.47058  ]\n",
      "   ...\n",
      "   [501.9608    313.7255    235.29411  ]\n",
      "   [576.4706    388.2353    301.9608   ]\n",
      "   [588.2353    400.        313.7255   ]]\n",
      "\n",
      "  [[525.4902    415.68628   368.62744  ]\n",
      "   [537.2549    427.451     380.39215  ]\n",
      "   [509.80392   392.15686   349.01962  ]\n",
      "   ...\n",
      "   [498.0392    298.0392    223.52942  ]\n",
      "   [517.64703   317.64706   243.13725  ]\n",
      "   [572.549     384.31372   298.0392   ]]\n",
      "\n",
      "  ...\n",
      "\n",
      "  [[796.0784    717.64703   713.72546  ]\n",
      "   [792.15686   713.72546   709.8039   ]\n",
      "   [792.15686   713.72546   709.8039   ]\n",
      "   ...\n",
      "   [494.11765   345.09805   333.33334  ]\n",
      "   [482.35294   337.2549    313.7255   ]\n",
      "   [474.5098    329.41177   298.0392   ]]\n",
      "\n",
      "  [[792.15686   713.72546   709.8039   ]\n",
      "   [792.15686   713.72546   709.8039   ]\n",
      "   [792.15686   713.72546   709.8039   ]\n",
      "   ...\n",
      "   [486.2745    337.2549    325.4902   ]\n",
      "   [470.58823   325.4902    294.11765  ]\n",
      "   [470.58823   317.64706   290.19608  ]]\n",
      "\n",
      "  [[792.15686   713.72546   709.8039   ]\n",
      "   [796.0784    717.64703   713.72546  ]\n",
      "   [796.0784    717.64703   713.72546  ]\n",
      "   ...\n",
      "   [486.2745    337.2549    325.4902   ]\n",
      "   [462.7451    317.64706   286.2745   ]\n",
      "   [466.66666   313.7255    286.2745   ]]]] [0. 1. 1. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0. 0. 1. 1. 1. 0. 1.\n",
      " 1. 1. 1. 0. 0. 1. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_gen:\n",
    "    print(x,y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model \n",
    "from keras.layers import BatchNormalization , Dense, SeparableConv2D ,MaxPool2D , Dropout ,Input ,Flatten\n",
    "\n",
    "inp = Input( shape=(224,224,3))\n",
    "x = SeparableConv2D(32 ,3 ,activation=\"relu\")(inp)\n",
    "x = SeparableConv2D(16, 3,activation= \"relu\" )(x)\n",
    "x = BatchNormalization(1)(x)\n",
    "x = MaxPool2D(2)(x)\n",
    "x = SeparableConv2D(8 , 3 ,activation=\"relu\")(x)\n",
    "x = SeparableConv2D(6 ,3 ,activation =\"relu\" )(x)\n",
    "x = MaxPool2D(2)(x)\n",
    "x = BatchNormalization(1,epsilon=1e-5)(x)\n",
    "x = Dropout(0.2)(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(16 , activation=\"relu\")(x)\n",
    "x = BatchNormalization()(x)\n",
    "out = Dense(1,activation = \"sigmoid\")(x)\n",
    "model = Model(inp , out)\n",
    "\n",
    "model.compile(loss = \"binary_crossentropy\",optimizer = \"rmsprop\",metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_9 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_44 (Separab (None, 222, 222, 32)      155       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_45 (Separab (None, 220, 220, 16)      816       \n",
      "_________________________________________________________________\n",
      "batch_normalization_26 (Batc (None, 220, 220, 16)      880       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_20 (MaxPooling (None, 110, 110, 16)      0         \n",
      "_________________________________________________________________\n",
      "separable_conv2d_46 (Separab (None, 108, 108, 8)       280       \n",
      "_________________________________________________________________\n",
      "separable_conv2d_47 (Separab (None, 106, 106, 6)       126       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_21 (MaxPooling (None, 53, 53, 6)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_27 (Batc (None, 53, 53, 6)         212       \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 53, 53, 6)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 16854)             0         \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 16)                269680    \n",
      "_________________________________________________________________\n",
      "batch_normalization_28 (Batc (None, 16)                64        \n",
      "_________________________________________________________________\n",
      "dense_17 (Dense)             (None, 1)                 17        \n",
      "=================================================================\n",
      "Total params: 272,230\n",
      "Trainable params: 271,652\n",
      "Non-trainable params: 578\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "125/125 [==============================] - 360s 3s/step - loss: 0.2217 - accuracy: 0.9233 - val_loss: 0.8411 - val_accuracy: 0.5000\n",
      "Epoch 2/20\n",
      "125/125 [==============================] - 365s 3s/step - loss: 0.1011 - accuracy: 0.9715 - val_loss: 0.8711 - val_accuracy: 0.5000\n",
      "Epoch 3/20\n",
      "125/125 [==============================] - 370s 3s/step - loss: 0.0499 - accuracy: 0.9865 - val_loss: 0.7880 - val_accuracy: 0.5033\n",
      "Epoch 4/20\n",
      "125/125 [==============================] - 336s 3s/step - loss: 0.0306 - accuracy: 0.9915 - val_loss: 0.1541 - val_accuracy: 0.9600\n",
      "Epoch 5/20\n",
      "125/125 [==============================] - 307s 2s/step - loss: 0.0252 - accuracy: 0.9915 - val_loss: 0.2443 - val_accuracy: 0.9433\n",
      "Epoch 6/20\n",
      "125/125 [==============================] - 305s 2s/step - loss: 0.0158 - accuracy: 0.9950 - val_loss: 0.6044 - val_accuracy: 0.8000\n",
      "Epoch 7/20\n",
      "125/125 [==============================] - 305s 2s/step - loss: 0.0202 - accuracy: 0.9930 - val_loss: 0.2145 - val_accuracy: 0.9567\n",
      "Epoch 8/20\n",
      "125/125 [==============================] - 347s 3s/step - loss: 0.0117 - accuracy: 0.9967 - val_loss: 0.1313 - val_accuracy: 0.9700\n",
      "Epoch 9/20\n",
      "125/125 [==============================] - 360s 3s/step - loss: 0.0141 - accuracy: 0.9960 - val_loss: 0.2243 - val_accuracy: 0.9700\n",
      "Epoch 10/20\n",
      "125/125 [==============================] - 367s 3s/step - loss: 0.0095 - accuracy: 0.9965 - val_loss: 0.2411 - val_accuracy: 0.9367\n",
      "Epoch 11/20\n",
      "125/125 [==============================] - 375s 3s/step - loss: 0.0132 - accuracy: 0.9952 - val_loss: 0.1628 - val_accuracy: 0.9600\n",
      "Epoch 12/20\n",
      "125/125 [==============================] - 387s 3s/step - loss: 0.0126 - accuracy: 0.9962 - val_loss: 0.2934 - val_accuracy: 0.9233\n",
      "Epoch 13/20\n",
      "125/125 [==============================] - 348s 3s/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.1759 - val_accuracy: 0.9700\n",
      "Epoch 14/20\n",
      "125/125 [==============================] - 359s 3s/step - loss: 0.0080 - accuracy: 0.9973 - val_loss: 0.4338 - val_accuracy: 0.8900\n",
      "Epoch 15/20\n",
      "125/125 [==============================] - 340s 3s/step - loss: 0.0094 - accuracy: 0.9973 - val_loss: 0.1551 - val_accuracy: 0.9800\n",
      "Epoch 16/20\n",
      "125/125 [==============================] - 331s 3s/step - loss: 0.0094 - accuracy: 0.9965 - val_loss: 0.9318 - val_accuracy: 0.8267\n",
      "Epoch 17/20\n",
      "125/125 [==============================] - 308s 2s/step - loss: 0.0064 - accuracy: 0.9967 - val_loss: 0.2537 - val_accuracy: 0.9667\n",
      "Epoch 18/20\n",
      "125/125 [==============================] - 305s 2s/step - loss: 0.0117 - accuracy: 0.9960 - val_loss: 0.1851 - val_accuracy: 0.9800\n",
      "Epoch 19/20\n",
      "125/125 [==============================] - 307s 2s/step - loss: 0.0069 - accuracy: 0.9977 - val_loss: 0.2231 - val_accuracy: 0.9600\n",
      "Epoch 20/20\n",
      "125/125 [==============================] - 306s 2s/step - loss: 0.0073 - accuracy: 0.9967 - val_loss: 0.1566 - val_accuracy: 0.9600\n"
     ]
    }
   ],
   "source": [
    "history_1 = model.fit(train_gen, epochs=20  , validation_data=(validation_gen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"A&B_class_Val_acc97.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 300 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_gen = test_data.flow_from_directory(\"test\",target_size=(224,224),class_mode='binary')\n",
    "\n",
    "\n",
    "testing = model.evaluate(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing  accuracy is  0.99 with loss  0.023090877003050993\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing  accuracy is \" , testing[1], \"with loss \", testing[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import image\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('float32')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "l_model = load_model(\"A&B_class_Val_acc97.h5\")\n",
    "img = image.load_img(\"b.png\",target_size=(224,224))\n",
    "img = image.img_to_array(img )\n",
    "pic  = img * 1./255\n",
    "pic.dtype\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pic=pic.reshape((1,224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual labels is ['Beautiful'] and predicted label is Beautiful\n"
     ]
    }
   ],
   "source": [
    "label = [\"Beautiful\"]\n",
    "pred = l_model.predict(pic)\n",
    "if pred < 0.5 :\n",
    "    Print(f\"Actual labels is {label} and predicted label is Average\")\n",
    "else :\n",
    "    print(f\"Actual labels is {label} and predicted label is Beautiful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
